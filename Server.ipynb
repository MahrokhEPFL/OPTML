{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "sys.path.append(os.getcwd()+'/assistive_functions')\n",
    "\n",
    "from SMWrapper import SMWrapper\n",
    "from household import Household, SyNet\n",
    "from load_data import get_data_of_a_person\n",
    "from construct_dataset import construct_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(30)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rubber-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vietnamese-rubber",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d43f038f552b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "test = torch.zeros((10,10))\n",
    "test.shape(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate lags\n",
    "def get_lags(step_ahead, num_days=np.array([0,1,7])):\n",
    "    lags = []\n",
    "    for i in num_days:\n",
    "        lags = lags + [48*i, 48*i+1, 48*i+2]\n",
    "    lags = [x-step_ahead+1 for x in lags if x>=step_ahead]\n",
    "    return lags\n",
    "\n",
    "def connect_to_households(household_options):\n",
    "    # get candidate houses from the selected group\n",
    "    path = os.getcwd()+\"/input/informations_households.csv.xls\"\n",
    "    data = pd.read_csv(path)\n",
    "    # filter by group\n",
    "    candidates = data.loc[data.Acorn==household_options['group']]\n",
    "    # filter by tariff \n",
    "    candidates = candidates.loc[candidates.stdorToU==household_options['stdorToU']]\n",
    "    # print(candidates)    \n",
    "    # TODO: shuffle\n",
    "    households=[]\n",
    "    step_ahead=1\n",
    "\n",
    "    # create households\n",
    "    needed = household_options['num_households']\n",
    "    num = 0\n",
    "    while needed>0:\n",
    "        # check if there are enough households\n",
    "        if num>=len(candidates):\n",
    "            num_households = len(households)\n",
    "            print('[Warning] could not find enough households')\n",
    "            print('[Warning] changed number of households to ' + str(num_households))\n",
    "        # get household\n",
    "        household = Household(house_id=candidates.LCLid.iloc[num],\n",
    "                               block_num=candidates.file.iloc[num])\n",
    "        # load data with regression options\n",
    "        household.construct_dataset(lags=get_lags(step_ahead), step_ahead=step_ahead, options=options)\n",
    "        if len(household.y) > 0:\n",
    "            households.append(household)\n",
    "            needed = needed-1\n",
    "        # search next\n",
    "        num = num+1\n",
    "\n",
    "    print('\\n[INFO] Connected to ' + str(len(households)) + ' households')\n",
    "    return households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of devices and their group\n",
    "num_households = 4\n",
    "group=\"ACORN-L\"\n",
    "stdorToU=\"ToU\"\n",
    "household_options = {\"num_households\":num_households,\n",
    "                    \"group\":group,\n",
    "                    \"stdorToU\":stdorToU}\n",
    "\n",
    "# regression options\n",
    "options = {\"dayparts\":[],\n",
    "           \"resolution\":60,\n",
    "           \"remove_holiday\":True,\n",
    "           \"filt_days\":['Tuesday'], \n",
    "           \"replacement_method\":'week_before',\n",
    "           \"feat_cols\":['hourofd_x', 'hourofd_y', 'dayofy_x', 'dayofy_y', 'temperature_hourly']}\n",
    "step_ahead=1\n",
    "\n",
    "households = connect_to_households(household_options)\n",
    "\n",
    "# construct dataset\n",
    "for household in households:\n",
    "    household.construct_dataset(lags=get_lags(step_ahead), step_ahead=step_ahead, options=options)\n",
    "    household.train_test_split(test_frac=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param(model, message=''):\n",
    "    with np.printoptions(precision=3, suppress=True):\n",
    "        print(message+' model: \\nbias = ' + \n",
    "              str(model.state_dict()['linear.bias'].numpy()[0]) +\n",
    "              ', \\nweights = ' + \n",
    "              str(model.state_dict()['linear.weight'].numpy().flatten()))\n",
    "    return\n",
    "        \n",
    "def compute_delta(household, args, output):\n",
    "    # run minibatch SGD and get update in parameters\n",
    "    model = args[0]\n",
    "    cur_state_dict = args[1]\n",
    "    optim = args[2]\n",
    "    mbsize = args[3]\n",
    "    tot_samp = args[4]\n",
    "    \n",
    "    \n",
    "    db, dw = household.minibatch_SGD(model=model,optim=optim, mbsize=mbsize, verbose=False)\n",
    "    # aggregate updates\n",
    "    delta_bias  = db*household.info['train_samples']/tot_samp\n",
    "    delta_weight= dw*household.info['train_samples']/tot_samp\n",
    "    print(delta_weight)\n",
    "    output.put((delta_weight, delta_bias))            \n",
    "\n",
    "def _read(q):\n",
    "    while True:\n",
    "        delta_weight, delta_bais = q.get(True)\n",
    "        \n",
    "def distributed_train_parallel(households, lr, mbsize, total_it, optim_method, **kwargs):\n",
    "    '''\n",
    "    optim_method: Adam or SGD\n",
    "    '''\n",
    "    random.seed(30)\n",
    "    np.random.seed(30)\n",
    "    torch.manual_seed(30)\n",
    "    # find total samples\n",
    "    tot_samp = 0\n",
    "    for household in households:\n",
    "        tot_samp = tot_samp + household.info['train_samples']\n",
    "    # initialize model\n",
    "    if 'init_state_dict' in kwargs:\n",
    "        set_init = True\n",
    "        init_state_dict = kwargs.get('init_state_dict')\n",
    "    else:\n",
    "        set_init = False\n",
    "    # create an initial model with random parameters\n",
    "    in_dim=households[0].info['num_features']\n",
    "    model = SyNet(torch, in_dim=in_dim, out_dim=1)\n",
    "    if set_init:\n",
    "        for key, value in init_state_dict.items():\n",
    "            model.state_dict()[key].copy_(value)\n",
    "        print_param(model, 'Initial DA ')\n",
    "    # create optimizer\n",
    "    if optim_method=='Adam':\n",
    "        optim = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    else:\n",
    "        if optim_method=='SGD':\n",
    "            optim = torch.optim.SGD(params=model.parameters(), momentum=0, lr=lr)\n",
    "        else:\n",
    "            print('Unsupported optimization method')\n",
    "            return\n",
    "        \n",
    "    time_start = time.time()\n",
    "    # iterate\n",
    "    for i in np.arange(math.floor(total_it/mbsize)):\n",
    "        # initialize param update\n",
    "        cur_state_dict=copy.deepcopy(model.state_dict())\n",
    "        cur_weight = model.state_dict()['linear.weight'].numpy().flatten()\n",
    "        cur_bias = model.state_dict()['linear.bias'].numpy()\n",
    "        delta_bias = np.zeros(1)\n",
    "        delta_weight = np.zeros(in_dim)\n",
    "        # run minibatch SGD for each household\n",
    "        arguments = [model, cur_state_dict, optim,mbsize, tot_samp]\n",
    "        results = mp.Queue(0)\n",
    "        pool = mp.Pool(processes=len(households))\n",
    "        for household in households:\n",
    "                pool.apply_async(compute_delta, args = (household, arguments, results))\n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        #missing a step to collect the weight and bias in the results queue\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update model\n",
    "        new_bias = cur_bias + delta_bias/len(households)\n",
    "        new_bias = torch.tensor(new_bias)\n",
    "        new_weight = cur_weight + delta_weight/len(households)\n",
    "        new_weight = torch.tensor(new_weight.reshape((1,len(new_weight))))\n",
    "        model.state_dict()['linear.weight'].copy_(new_weight)\n",
    "        model.state_dict()['linear.bias'].copy_(new_bias)\n",
    "        #print_param(model, 'Iteration ' + str(i) + ' ')\n",
    "\n",
    "        #print(' ')\n",
    "        \n",
    "    time_end = time.time()\n",
    "    print('elapsed time: ',time_end-time_start)\n",
    "    # print trained parameters\n",
    "    print_param(model, 'Trained DA ')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param(model, message=''):\n",
    "    with np.printoptions(precision=3, suppress=True):\n",
    "        print(message+' model: \\nbias = ' + \n",
    "              str(model.state_dict()['linear.bias'].numpy()[0]) +\n",
    "              ', \\nweights = ' + \n",
    "              str(model.state_dict()['linear.weight'].numpy().flatten()))\n",
    "    return\n",
    "        \n",
    "        \n",
    "def distributed_train(households, lr, mbsize, total_it, optim_method, **kwargs):\n",
    "    '''\n",
    "    optim_method: Adam or SGD\n",
    "    '''\n",
    "    random.seed(30)\n",
    "    np.random.seed(30)\n",
    "    torch.manual_seed(30)\n",
    "    # find total samples\n",
    "    tot_samp = 0\n",
    "    for household in households:\n",
    "        tot_samp = tot_samp + household.info['train_samples']\n",
    "    # initialize model\n",
    "    if 'init_state_dict' in kwargs:\n",
    "        set_init = True\n",
    "        init_state_dict = kwargs.get('init_state_dict')\n",
    "    else:\n",
    "        set_init = False\n",
    "    # create an initial model with random parameters\n",
    "    in_dim=households[0].info['num_features']\n",
    "    model = SyNet(torch, in_dim=in_dim, out_dim=1)\n",
    "    if set_init:\n",
    "        for key, value in init_state_dict.items():\n",
    "            model.state_dict()[key].copy_(value)\n",
    "        print_param(model, 'Initial DA ')\n",
    "    # create optimizer\n",
    "    if optim_method=='Adam':\n",
    "        optim = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    else:\n",
    "        if optim_method=='SGD':\n",
    "            optim = torch.optim.SGD(params=model.parameters(), momentum=0, lr=lr)\n",
    "        else:\n",
    "            print('Unsupported optimization method')\n",
    "            return\n",
    "        \n",
    "    time_start = time.time()\n",
    "    # iterate\n",
    "    for i in np.arange(math.floor(total_it/mbsize)):\n",
    "        # initialize param update\n",
    "        cur_state_dict=copy.deepcopy(model.state_dict())\n",
    "        cur_weight = model.state_dict()['linear.weight'].numpy().flatten()\n",
    "        cur_bias = model.state_dict()['linear.bias'].numpy()\n",
    "        delta_bias = np.zeros(1)\n",
    "        delta_weight = np.zeros(in_dim)\n",
    "        # run minibatch SGD for each household\n",
    "        for household in households:\n",
    "            # run minibatch SGD and get update in parameters\n",
    "            db, dw = household.minibatch_SGD(model=model,optim=optim, mbsize=mbsize, verbose=False)\n",
    "            # aggregate updates\n",
    "            delta_bias  = db*household.info['train_samples']/tot_samp + delta_bias\n",
    "            delta_weight= dw*household.info['train_samples']/tot_samp + delta_weight\n",
    "            # reset model\n",
    "            for key, value in cur_state_dict.items():\n",
    "                model.state_dict()[key].copy_(value)\n",
    "        # update model\n",
    "        print(delta_weight)\n",
    "        new_bias = cur_bias + delta_bias/len(households)\n",
    "        new_bias = torch.tensor(new_bias)\n",
    "        new_weight = cur_weight + delta_weight/len(households)\n",
    "        new_weight = torch.tensor(new_weight.reshape((1,len(new_weight))))\n",
    "        model.state_dict()['linear.weight'].copy_(new_weight)\n",
    "        model.state_dict()['linear.bias'].copy_(new_bias)\n",
    "        #print_param(model, 'Iteration ' + str(i) + ' ')\n",
    "\n",
    "        #print(' ')\n",
    "    # print trained parameters\n",
    "    time_end = time.time()\n",
    "    print('elapsed time: ',time_end-time_start)\n",
    "    print_param(model, 'Trained DA ')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "mbsize = 2\n",
    "total_it = 400\n",
    "\n",
    "# create an initial model with random parameters\n",
    "model = SyNet(torch, in_dim=household.info['num_features'], out_dim=1)\n",
    "init_state_dict = copy.deepcopy(model.state_dict())\n",
    "  \n",
    "# set training data\n",
    "households[0].train_test_split(test_frac=0.25)    \n",
    "\n",
    "# METHOD 1\n",
    "household = households[0]\n",
    "print('\\nMethod 1')\n",
    "household.fit_personal_model(method='Adam', lr=lr, iterations=total_it, init_state_dict=init_state_dict)\n",
    "model_lr = household.personal_lr\n",
    "print_param(model_lr, 'personal')\n",
    "\n",
    "# METHOD 2\n",
    "print('\\nMethod 2')\n",
    "model_da = distributed_train_parallel([household], optim_method='Adam', \n",
    "                             lr=lr, mbsize=mbsize, total_it=total_it, \n",
    "                             init_state_dict=init_state_dict)\n",
    "\n",
    "# Compare\n",
    "tol = 1e-3\n",
    "dif_b = np.abs(model_da.state_dict()['linear.bias'].numpy()-model_lr.state_dict()['linear.bias'].numpy())\n",
    "dif_w = np.abs(model_da.state_dict()['linear.weight'].numpy().flatten()-model_lr.state_dict()['linear.weight'].numpy().flatten())\n",
    "if np.all(dif_b<tol) and np.all(dif_w<tol):\n",
    "    print('\\nSingle household check was successful.')\n",
    "else:\n",
    "    print('\\nSingle household check failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "mbsize = 2\n",
    "total_it = 400\n",
    "\n",
    "# create an initial model with random parameters\n",
    "model = SyNet(torch, in_dim=household.info['num_features'], out_dim=1)\n",
    "init_state_dict = copy.deepcopy(model.state_dict())\n",
    "  \n",
    "# set training data\n",
    "households[0].train_test_split(test_frac=0.25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "mbsize = 2\n",
    "total_it = 400\n",
    "\n",
    "# create an initial model with random parameters\n",
    "model = SyNet(torch, in_dim=household.info['num_features'], out_dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work] *",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
