{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook for testing Mocha\n",
    "\n",
    "## 0. Load packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "from numpy import matlib as mb\n",
    "sys.path.append(os.getcwd()+'/assistive_functions')\n",
    "\n",
    "from household import Household, SyNet\n",
    "from utils_households import get_lags, connect_to_households"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load households data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Connected to 2 households\n"
     ]
    }
   ],
   "source": [
    "# number of devices and their group\n",
    "num_households = 2\n",
    "group=\"ACORN-L\"\n",
    "stdorToU=\"ToU\"\n",
    "household_options = {\"num_households\":num_households,\n",
    "                    \"group\":group,\n",
    "                    \"stdorToU\":stdorToU}\n",
    "\n",
    "households = connect_to_households(household_options)\n",
    "\n",
    "\n",
    "\n",
    "# regression options\n",
    "options = {\"dayparts\":[],\n",
    "           \"resolution\":60,\n",
    "           \"remove_holiday\":True,\n",
    "           \"filt_days\":['Tuesday'], \n",
    "           \"replacement_method\":'week_before',\n",
    "           \"feat_cols\":['hourofd_x', 'hourofd_y', 'dayofy_x', 'dayofy_y', 'temperature_hourly']}\n",
    "step_ahead=1\n",
    "\n",
    "# find dates when all households had data\n",
    "date_st = []\n",
    "date_en = []\n",
    "for household in households:\n",
    "    household.construct_dataset(lags=get_lags(step_ahead), step_ahead=step_ahead, options=options)\n",
    "    date_st.append(household.cons_data.date.iloc[0])\n",
    "    date_en.append(household.cons_data.date.iloc[-1])\n",
    "date_st_com = max(date_st)\n",
    "date_en_com = min(date_en)\n",
    "\n",
    "# construct dataset\n",
    "for household in households:\n",
    "    household.construct_dataset(lags=get_lags(step_ahead), step_ahead=step_ahead, options=options, \n",
    "                                date_st=date_st_com, date_en=date_en_com)\n",
    "    household.train_test_split(test_frac=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean-Regularized MTL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param(model, message=''):\n",
    "    with np.printoptions(precision=3, suppress=True):\n",
    "        print(message+' model: \\nbias = ' + \n",
    "              str(model.state_dict()['linear.bias'].numpy()[0]) +\n",
    "              ', \\nweights = ' + \n",
    "              str(model.state_dict()['linear.weight'].numpy().flatten()))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_av_weights(households, **kwargs):\n",
    "    models = kwargs.get('models', [household.model_mtl for household in households])\n",
    "    in_dim = households[0].info['num_features']\n",
    "    # find total samples\n",
    "    tot_samp = 0\n",
    "    for household in households:\n",
    "        tot_samp = tot_samp + household.info['train_samples']\n",
    "    # init\n",
    "    bias = np.zeros(1)\n",
    "    wght = np.zeros(in_dim)\n",
    "    # aggregate    \n",
    "    for model in models:\n",
    "        # aggregate updates\n",
    "        bias_hh = model.state_dict()['linear.bias'].numpy()\n",
    "        wght_hh = model.state_dict()['linear.weight'].numpy().flatten()\n",
    "        bias = bias_hh*household.info['train_samples']/tot_samp + bias\n",
    "        wght = wght_hh*household.info['train_samples']/tot_samp + wght \n",
    "    return bias, wght\n",
    "\n",
    "    \n",
    "def mtl_train(households, lr, lambda_, inner_iters, outer_iters, optim_method, **kwargs):\n",
    "    '''\n",
    "    optim_method: Adam or SGD\n",
    "    '''\n",
    "    np.random.seed(30)\n",
    "    torch.manual_seed(30)\n",
    "    \n",
    "    verbose = kwargs.get('verbose', False)\n",
    "    # find total samples\n",
    "    tot_samp = 0\n",
    "    for household in households:\n",
    "        tot_samp = tot_samp + household.info['train_samples']\n",
    "    \n",
    "    \n",
    "    # initialize mtl for households\n",
    "    for household in households:\n",
    "        household.mtl_init(lr)\n",
    "    # initialize w_0\n",
    "    in_dim=households[0].info['num_features']\n",
    "    w_0 = SyNet(torch, in_dim=in_dim, out_dim=1)\n",
    "    # find av initial weights\n",
    "    init_bias, init_wght = get_av_weights(households)\n",
    "    w_0.state_dict()['linear.weight'].copy_(torch.tensor(init_wght.reshape((1,len(init_wght)))))\n",
    "    w_0.state_dict()['linear.bias'].copy_(torch.tensor(init_bias))\n",
    "    init_state_dict=copy.deepcopy(w_0.state_dict())\n",
    "    \n",
    "    # create optimizer\n",
    "    if optim_method=='Adam':\n",
    "        optim = torch.optim.Adam(params=w_0.parameters(), lr=lr)\n",
    "    else:\n",
    "        if optim_method=='SGD':\n",
    "            optim = torch.optim.SGD(params=w_0.parameters(), momentum=0, lr=lr)\n",
    "        else:\n",
    "            print('Unsupported optimization method')\n",
    "            return\n",
    "    \n",
    "    \n",
    "    # iterate\n",
    "    for i in np.arange(outer_iters):\n",
    "        if verbose:\n",
    "            print('\\n before iter ' + str(i))\n",
    "            print_param(w_0, 'w_0 ')\n",
    "            \n",
    "        # initialize param update\n",
    "        cur_state_dict=copy.deepcopy(w_0.state_dict())\n",
    "        w_0_wght = w_0.state_dict()['linear.weight']\n",
    "        w_0_bias = w_0.state_dict()['linear.bias']\n",
    "        delta_bias = np.zeros(1)\n",
    "        delta_wght = np.zeros(in_dim)\n",
    "        # run minibatch SGD for each household\n",
    "        for household in households:\n",
    "            # run minibatch SGD and get update in parameters\n",
    "            db, dw = household.mtl_iterate(w_0_wght=w_0_wght, w_0_bias=w_0_bias,\n",
    "                                           inner_iters=inner_iters, \n",
    "                                           lambda_=lambda_, verbose=False)\n",
    "            # aggregate updates\n",
    "            delta_bias = db*household.info['train_samples']/tot_samp + delta_bias\n",
    "            delta_wght = dw*household.info['train_samples']/tot_samp + delta_wght\n",
    "            # reset w_0\n",
    "            for key, value in cur_state_dict.items():\n",
    "                w_0.state_dict()[key].copy_(value)\n",
    "        # update w_0\n",
    "        new_bias = w_0_bias + torch.tensor(delta_bias.reshape((1)))\n",
    "        new_wght = w_0_wght + torch.tensor(delta_wght.reshape((1,in_dim)))\n",
    "        w_0.state_dict()['linear.weight'].copy_(new_wght)\n",
    "        w_0.state_dict()['linear.bias'].copy_(new_bias)\n",
    "        #print_param(w_0, 'Iteration ' + str(i) + ' ')\n",
    "\n",
    "        #print(' ')\n",
    "    # print trained parameters\n",
    "    print_param(w_0, 'Trained w_0 ')\n",
    "    return w_0, init_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained w_0  model: \n",
      "bias = 0.27035052, \n",
      "weights = [-0.113 -0.017 -0.024  0.004 -0.113  1.325 -0.008  0.32   0.048 -0.09\n",
      "  0.513  0.02  -0.062]\n"
     ]
    }
   ],
   "source": [
    "inner_iters = 20\n",
    "outer_iters = 20\n",
    "lr = 0.1\n",
    "lambda_ = 10\n",
    "\n",
    "w_0, init_state_dict = mtl_train(households, lr=lr, lambda_=lambda_, \n",
    "          inner_iters=inner_iters, outer_iters=outer_iters, \n",
    "          optim_method='Adam', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanity Check\n",
    "This section is for checking if the distributed MTL algorithm works correctly by experimenting with two extreme cases, when lambda=0 and lambda is very large. \n",
    "\n",
    "### 3.1 No regularization\n",
    "First, we set lambda=0 to remove the regularization term. In this scenario, the MTL algorithm must return the same models as if each household fits a personal model on its' own local dataset. We run two tests: 1) apply MTL with lambda=0, 2) fit linear models to local datasets separately. If initiated at the same parameters, both methods return the same weights and bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained w_0  model: \n",
      "bias = 0.27035052, \n",
      "weights = [-0.113 -0.017 -0.024  0.004 -0.113  1.325 -0.008  0.32   0.048 -0.09\n",
      "  0.513  0.02  -0.062]\n",
      "\n",
      "Sanity check lambda=0 was successful.\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: MTL\n",
    "inner_iters = 20\n",
    "outer_iters = 20\n",
    "lr = 0.1\n",
    "lambda_ = 0\n",
    "w_0, init_state_dict = mtl_train(households, lr=lr, lambda_=lambda_, \n",
    "          inner_iters=inner_iters, outer_iters=outer_iters, \n",
    "          optim_method='Adam', verbose=False)\n",
    "\n",
    "# METHOD 2: separate local models\n",
    "for household in households:\n",
    "    household.fit_personal_model(method='Adam', lr=lr, \n",
    "                                 iterations = inner_iters*outer_iters, \n",
    "                                 init_params=init_state_dict)\n",
    "\n",
    "# Compare\n",
    "tol = 1e-3\n",
    "# compare mean model\n",
    "av_bias, av_wght = get_av_weights(households, models = [household.personal_lr for household in households]) \n",
    "w_0_bias = w_0.state_dict()['linear.bias'].numpy()\n",
    "w_0_wght = w_0.state_dict()['linear.weight'].numpy().flatten()\n",
    "dff_bias = np.abs(av_bias - w_0_bias)\n",
    "dff_wght = np.abs(av_wght - w_0_wght)\n",
    "if np.all(dff_bias<tol) and np.all(dff_wght<tol):\n",
    "    matched=True\n",
    "else:\n",
    "    matched=False\n",
    "    print('\\nSanity check lambda=0 failed.')\n",
    "    \n",
    "# compare personal models\n",
    "if matched:\n",
    "    for household in households:\n",
    "        plr_bias = household.personal_lr.state_dict()['linear.bias'].numpy()\n",
    "        plr_wght = household.personal_lr.state_dict()['linear.weight'].numpy().flatten()\n",
    "        mtl_bias = household.model_mtl.state_dict()['linear.bias'].numpy()\n",
    "        mtl_wght = household.model_mtl.state_dict()['linear.weight'].numpy().flatten()\n",
    "        dff_bias = np.abs(plr_bias - mtl_bias)\n",
    "        dff_wght = np.abs(plr_wght - mtl_wght)\n",
    "        if np.all(dff_bias<tol) and np.all(dff_wght<tol):\n",
    "            matched=True\n",
    "        else:\n",
    "            matched=False\n",
    "            print('\\nSanity check lambda=0 failed.')\n",
    "            break\n",
    "    if matched:\n",
    "        print('\\nSanity check lambda=0 was successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Highly regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained w_0  model: \n",
      "bias = 0.14246745, \n",
      "weights = [ 0.045  0.008  0.155  0.014 -0.057  0.116  0.271 -0.039 -0.073  0.183\n",
      "  0.106  0.019  0.082]\n",
      " model: \n",
      "bias = 0.11399404, \n",
      "weights = [ 0.021  0.042  0.191  0.022 -0.044  0.097  0.333 -0.013 -0.071  0.174\n",
      "  0.102  0.026  0.052]\n",
      "\n",
      "Sanity check lambda=inf failed.\n"
     ]
    }
   ],
   "source": [
    "inner_iters = 2000\n",
    "outer_iters = 1\n",
    "lr = 1\n",
    "lambda_ = 1000\n",
    "w_0, init_state_dict = mtl_train(households, lr=lr, lambda_=lambda_, \n",
    "          inner_iters=inner_iters, outer_iters=outer_iters, \n",
    "          optim_method='Adam', verbose=False)\n",
    "\n",
    "# Compare\n",
    "tol = 1e-2\n",
    "w_0_bias = w_0.state_dict()['linear.bias'].numpy()\n",
    "w_0_wght = w_0.state_dict()['linear.weight'].numpy().flatten()\n",
    "for household in households:\n",
    "    print_param(household.model_mtl)\n",
    "    mtl_bias = household.model_mtl.state_dict()['linear.bias'].numpy()\n",
    "    mtl_wght = household.model_mtl.state_dict()['linear.weight'].numpy().flatten()\n",
    "    dff_bias = np.abs(w_0_bias - mtl_bias)\n",
    "    dff_wght = np.abs(w_0_wght - mtl_wght)\n",
    "    if np.all(dff_bias<tol) and np.all(dff_wght<tol):\n",
    "        matched=True\n",
    "    else:\n",
    "        matched=False\n",
    "        print('\\nSanity check lambda=inf failed.')\n",
    "        break\n",
    "if matched:\n",
    "    print('\\nSanity check lambda=inf was successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Central Training\n",
    "Problem setup:\n",
    "* Training data from all households is available simultaneously at a data center \n",
    "\n",
    "Algorithm:\n",
    "* Training datasets from all households are augmented into a single matrix\n",
    "* Training iterations are executed on the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_train(households, optim_method, lr, lambda_, total_it, **kwargs):\n",
    "    np.random.seed(30)\n",
    "    torch.manual_seed(30)\n",
    "    \n",
    "    # find total samples\n",
    "    tot_samp = 0\n",
    "    for household in households:\n",
    "        tot_samp = tot_samp + household.info['train_samples']\n",
    "    d = households[0].info['num_features']\n",
    "    N = len(households)\n",
    "    C = mb.repmat(np.eye(d), N, N)\n",
    "    X_train_aug = np.zeros((tot_samp, N*d))\n",
    "    y_train_aug = np.zeros((tot_samp, 1))\n",
    "    ind = 0\n",
    "    # Augment training datasets\n",
    "    for h_ind, household in enumerate(households):\n",
    "        num = household.info['train_samples']\n",
    "        # feat cols\n",
    "        X_train_aug[ind:ind+num, h_ind*d:h_ind*d+d] = household.X_train\n",
    "        y_train_aug[ind:ind+num,:] = household.y_train\n",
    "    \n",
    "    if 'init_state_dict' in kwargs:\n",
    "        set_init = True\n",
    "        init_state_dict = kwargs.get('init_state_dict')\n",
    "    else:\n",
    "        set_init = False\n",
    "    # create an initial model with random parameters\n",
    "    in_dim=households[0].info['num_features']\n",
    "    model = SyNet(torch, in_dim=in_dim*N, out_dim=1)\n",
    "    if set_init:\n",
    "        for key, value in init_state_dict.items():\n",
    "            model.state_dict()[key].copy_(value)\n",
    "        #print_param(model, 'Initial CA ')\n",
    "    # create optimizer\n",
    "    if optim_method=='Adam':\n",
    "        optim = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    else:\n",
    "        if optim_method=='SGD':\n",
    "            optim = torch.optim.SGD(params=model.parameters(), momentum=0, lr=lr)\n",
    "        else:\n",
    "            print('Unsupported optimization method')\n",
    "            return\n",
    "        \n",
    "    # iterate\n",
    "    for i in range(total_it):\n",
    "        optim.zero_grad()\n",
    "        # predict\n",
    "        output = model(torch.FloatTensor(X_train_aug))\n",
    "        # calculate loss\n",
    "        loss = torch.nn.MSELoss()(output, torch.FloatTensor(y_train_aug.reshape(-1, 1)))\n",
    "        #W = np.hstack((cur_state_dict['linear.bias'].numpy(), cur_state_dict['linear.weight'].numpy().flatten()))\n",
    "        curr_wght = model.parameters()[0]\n",
    "        curr_bias = model.parameters()[1]\n",
    "        W = curr_wght.detach().numpy().transpose()\n",
    "        W_bar = torch.FloatTensor(np.matmul(C,W))\n",
    "        l2_reg = torch.tensor(0.)\n",
    "        # only regularize weights not the bias\n",
    "        l2_reg += torch.norm(curr_wght-W_bar)\n",
    "        loss += lambda_ * l2_reg\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #print(\"Epoch \", i, \" train loss\", loss.item())\n",
    "\n",
    "    # print trained parameters\n",
    "    print_param(model, 'Trained CA ')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_it=10\n",
    "#total_it=inner_iters*outer_iters\n",
    "w_cent = central_train(households, optim_method='Adam', \n",
    "              lr=lr, lambda_=10, total_it=total_it, verbose=True)\n",
    "\n",
    "weights_cent_all = copy.deepcopy(w_cent.state_dict())['linear.weight'].numpy().flatten()\n",
    "in_dim = households[0].info['num_features']\n",
    "w_0_cent = np.zeros(in_dim)\n",
    "for h_ind in np.arange(len(households)):\n",
    "    w_0_cent = w_0_cent + weights_cent_all[h_ind*in_dim:(h_ind+1)*in_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
